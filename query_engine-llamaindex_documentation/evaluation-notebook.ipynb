{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## LlamaIndex Bottoms-Up Development - Evaluation Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62d9e62d0bf3f545"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Setup\n",
    "\"\"\"\n",
    "dotenv.load_dotenv('../.env')\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:25:49.984848Z",
     "start_time": "2024-02-27T12:25:49.721669Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from custom.markdown_docs_reader import MarkdownDocsReader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load Markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath, \n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "    \n",
    "    documents = loader.load_data()\n",
    "    \n",
    "    # exclude some metadata from the LLM\n",
    "    for document in documents:\n",
    "        document.excluded_llm_metadata_keys = [\"File Name\", \"Content Type\", \"Header Path\"]\n",
    "        \n",
    "    return documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:29:21.135944Z",
     "start_time": "2024-02-27T12:29:20.028367Z"
    }
   },
   "id": "c4d1a2be49d3ffa6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load our documents from each folder.\n",
    "# we keep them separate for now, in order to create seperate indexes later\n",
    "getting_started_docs = load_markdown_docs(\"data/docs/getting_started\")\n",
    "community_docs = load_markdown_docs(\"data/docs/community\")\n",
    "data_docs = load_markdown_docs(\"data/docs/core_modules/data_modules\")\n",
    "agent_docs = load_markdown_docs(\"data/docs/core_modules/agent_modules\")\n",
    "model_docs = load_markdown_docs(\"data/docs/core_modules/model_modules\")\n",
    "query_docs = load_markdown_docs(\"data/docs/core_modules/query_modules\")\n",
    "supporting_docs = load_markdown_docs(\"data/docs/core_modules/supporting_modules\")\n",
    "tutorials_docs = load_markdown_docs(\"data/docs/end_to_end_tutorials\")\n",
    "contributing_docs = load_markdown_docs(\"data/docs/development\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:30:16.021856Z",
     "start_time": "2024-02-27T12:30:15.940031Z"
    }
   },
   "id": "89ffac553a9c28d5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Indexes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb38be0f42f24a60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/johan/Desktop/Development/PycharmProjects/LlamaIndex-experiments/query_engine-llamaindex_documentation/data/storage/getting_started_index/docstore.json'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "\n",
    "# create a vector store index for each folder\n",
    "try:\n",
    "    getting_started_index = load_index_from_storage(StorageContext.from_defaults(persist_dir='data/storage/getting_started_index'))\n",
    "    community_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/community_index\"))\n",
    "    data_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/data_index\"))\n",
    "    agent_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/agent_index\"))\n",
    "    model_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/model_index\"))\n",
    "    query_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/query_index\"))\n",
    "    supporting_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/supporting_index\"))\n",
    "    tutorials_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/tutorials_index\"))\n",
    "    contributing_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"data/storage/contributing_index\"))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n",
    "    getting_started_index.storage_context.persist(persist_dir=\"data/storage/getting_started_index\")\n",
    "    \n",
    "    community_index = VectorStoreIndex.from_documents(community_docs)\n",
    "    community_index.storage_context.persist(persist_dir=\"data/storage/community_index\")\n",
    "\n",
    "    data_index = VectorStoreIndex.from_documents(data_docs)\n",
    "    data_index.storage_context.persist(persist_dir=\"data/storage/data_index\")\n",
    "\n",
    "    agent_index = VectorStoreIndex.from_documents(agent_docs)\n",
    "    agent_index.storage_context.persist(persist_dir=\"data/storage/agent_index\")\n",
    "\n",
    "    model_index = VectorStoreIndex.from_documents(model_docs)\n",
    "    model_index.storage_context.persist(persist_dir=\"data/storage/model_index\")\n",
    "\n",
    "    query_index = VectorStoreIndex.from_documents(query_docs)\n",
    "    query_index.storage_context.persist(persist_dir=\"data/storage/query_index\")    \n",
    "\n",
    "    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n",
    "    supporting_index.storage_context.persist(persist_dir=\"data/storage/supporting_index\")\n",
    "\n",
    "    tutorials_index = VectorStoreIndex.from_documents(tutorials_docs)\n",
    "    tutorials_index.storage_context.persist(persist_dir=\"data/storage/tutorials_index\")\n",
    "\n",
    "    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n",
    "    contributing_index.storage_context.persist(persist_dir=\"data/storage/contributing_index\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:37:19.749959Z",
     "start_time": "2024-02-27T12:36:28.300322Z"
    }
   },
   "id": "ead23b0f20fbcd38",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Query Engine Tool\n",
    "Since we have so many indexes, we can create a query engine tool for each and then use them in a single query engine!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd1409e1fd5734de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# create a query engine tool for each folder\n",
    "getting_started_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=getting_started_index.as_query_engine(), \n",
    "    name=\"Getting Started\", \n",
    "    description=\"Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works.\"\n",
    ")\n",
    "\n",
    "community_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=community_index.as_query_engine(),\n",
    "    name=\"Community\",\n",
    "    description=\"Useful for answering questions about integrations and other apps built by the community.\"\n",
    ")\n",
    "\n",
    "data_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_index.as_query_engine(),\n",
    "    name=\"Data Modules\",\n",
    "    description=\"Useful for answering questions about data loaders, documents, nodes, and index structures.\"\n",
    ")\n",
    "\n",
    "agent_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=agent_index.as_query_engine(),\n",
    "    name=\"Agent Modules\",\n",
    "    description=\"Useful for answering questions about data agents, agent configurations, and tools.\"\n",
    ")\n",
    "\n",
    "model_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=model_index.as_query_engine(),\n",
    "    name=\"Model Modules\",\n",
    "    description=\"Useful for answering questions about using and configuring LLMs, embedding modles, and prompts.\"\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_index.as_query_engine(),\n",
    "    name=\"Query Modules\",\n",
    "    description=\"Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline.\"\n",
    ")\n",
    "\n",
    "supporting_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=supporting_index.as_query_engine(),\n",
    "    name=\"Supporting Modules\",\n",
    "    description=\"Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation.\"\n",
    ")\n",
    "\n",
    "tutorials_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=tutorials_index.as_query_engine(),\n",
    "    name=\"Tutorials\",\n",
    "    description=\"Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases.\"\n",
    ")\n",
    "\n",
    "contributing_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=contributing_index.as_query_engine(),\n",
    "    name=\"Contributing\",\n",
    "    description=\"Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:41:09.983841Z",
     "start_time": "2024-02-27T12:41:09.977689Z"
    }
   },
   "id": "756eefc6e4680a42",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Unified Query Engine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59a98f372154523d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        getting_started_tool,\n",
    "        community_tool,\n",
    "        data_tool,\n",
    "        agent_tool,\n",
    "        model_tool,\n",
    "        query_tool,\n",
    "        supporting_tool,\n",
    "        tutorials_tool,\n",
    "        contributing_tool\n",
    "    ],\n",
    "    # enable this for streaming\n",
    "    # response_synthesizer=get_response_synthesizer(streaming=True),\n",
    "    verbose=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:45:40.134744Z",
     "start_time": "2024-02-27T12:45:39.850325Z"
    }
   },
   "id": "81306672b1efff74",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test the Query Engine!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cab63d9f3fcce1ff"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install llama index, you can use the following command: pip install llama-index.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I install llama index?\")\n",
    "print(str(response))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:46:24.519249Z",
     "start_time": "2024-02-27T12:46:18.961781Z"
    }
   },
   "id": "203947ed8b71c70e",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Baseline!\n",
    "\n",
    "Now that we have our baseline query engine created, we can create a basic evaluation pipleine.\n",
    "\n",
    "Our pipeline will:\n",
    "- Generate a small dataset of questions\n",
    "- Save/cache these questions (so we can properly compare performance later!)\n",
    "- Evaluate both response quality and hallucination\n",
    "\n",
    "To do this reliably, we need to use an LLM smarter that `gpt-3.5-turbo`, so we will setup `gpt-4` for the evaluation process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69184cbab5dc0af4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate the Dataset\n",
    "\n",
    "In order to make the question generation more efficient, we can remove small documents and combine all documents into a giant single document.\n",
    "\n",
    "I also modified the question generation prompt, to generate a single question for each chunk, along with extra context for what it is reading."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "738481301ace1323"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/docs\", recursive=True, required_exts=[\".md\"]).load_data()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for doc in documents:\n",
    "    all_text += doc.text\n",
    "    \n",
    "giant_document = Document(text=all_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:52:43.240841Z",
     "start_time": "2024-02-27T12:52:43.186929Z"
    }
   },
   "id": "598f6cb9a438c438",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from llama_index.core.indices.service_context import ServiceContext\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.prompts import Prompt\n",
    "from llama_index.llms import openai\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "\n",
    "llm = openai.OpenAI(llm=\"gpt-4\", temperature=0)\n",
    "\n",
    "question_dataset = []\n",
    "if os.path.exists(\"data/question_dataset.txt\"):\n",
    "    with open(\"data/question_dataset.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            question_dataset.append(line.strip())\n",
    "else:\n",
    "    # generate questions\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        [giant_document],\n",
    "        text_question_template=Prompt(\n",
    "            \"A sample from the LlamaIndex documentation is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "            \"{query_str}\"\n",
    "        ),\n",
    "        question_gen_query=(\n",
    "            \"You are an evaluator for a search pipeline. Your task is to write a single question \"\n",
    "            \"using the provided documentation sample above to test the search pipeline. The question should \"\n",
    "            \"reference specific names, functions, and terms. Restrict the question to the \"\n",
    "            \"context information provided.\\n\"\n",
    "            \"Question: \"\n",
    "        ),\n",
    "        # set this to be low, so we can generate more questions\n",
    "        # service_context=gpt4_service_context\n",
    "        llm=llm\n",
    "    )\n",
    "    generated_questions = data_generator.generate_questions_from_nodes()\n",
    "\n",
    "    # randomly pick 40 questions from each dataset\n",
    "    generated_questions = random.sample(generated_questions, 40)\n",
    "    question_dataset.extend(generated_questions)\n",
    "\n",
    "    print(f\"Generated {len(question_dataset)} questions.\")\n",
    "\n",
    "    # save the questions!\n",
    "    with open(\"data/question_dataset.txt\", \"w\") as f:\n",
    "        for question in question_dataset:\n",
    "            f.write(f\"{question.strip()}\\n\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:43:39.636100Z",
     "start_time": "2024-02-27T13:43:39.633034Z"
    }
   },
   "id": "6a62d90a5d0fcea1",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['What are the possible settings for the LLM and how can the user set the prompt for term extraction?',\n 'How can I convert tools to LangChain tools using the provided documentation sample?',\n 'What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?',\n 'What is the purpose of the SubQuestionQueryEngine class in LlamaIndex?',\n 'What is the purpose of the `query_wrapper_prompt` in the `HuggingFaceLLM` class?',\n 'What embedding model does LlamaIndex use by default?',\n 'What are the available options for the storage backend of the index store in LlamaIndex?',\n 'What is the function used to specify the metadata visible to the embedding model and how can it be customized?',\n 'What are the node postprocessors available in the LlamaIndex documentation?',\n 'What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?',\n 'What is the purpose of the DEFAULT_REFINE_PROMPT_SEL_LC in the LlamaIndex documentation?',\n \"What is the purpose of the `CollectionQueryConsumer` class in the Delphic application's WebSocket handling?\",\n 'What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?',\n 'How can I create a Django superuser using the Delphic application?',\n 'What is the default number of LLM calls required for the ListIndex?',\n 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?',\n 'What are the key building blocks in LlamaIndex for composing a Retrieval Augmented Generation (RAG) pipeline?',\n 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?',\n 'What is the purpose of the `VectorStoreIndex` class in the LlamaIndex documentation sample?',\n 'What storage backends are supported by LlamaIndex for persisting data?',\n 'What is the purpose of the `fetchDocuments` function in the `fetchDocuments.tsx` file in the React frontend?',\n 'What is the purpose of the `DecomposeQueryTransform` module in the LlamaIndex documentation?',\n 'What is the purpose of the `RefinePrompt` class in the LlamaIndex documentation?',\n \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\",\n 'What is the default value for the `include_metadata` parameter in the `SimpleNodeParser` class?',\n 'What is the purpose of the `RouterQueryEngine` in the LlamaIndex documentation?',\n 'What is the purpose of the TokenCountingHandler callback in the LlamaIndex library?',\n 'What is the purpose of the `RelatedNodeInfo` class in the LlamaIndex documentation?',\n 'What is the purpose of the ResponseEvaluator class in the LlamaIndex library?',\n 'What is the purpose of the `SimilarityPostprocessor` in the query engine configuration?',\n 'What is the purpose of the Algovera tool built on top of LlamaIndex?',\n 'How can you delete a document from the index data structures and specify whether to delete it from the document store as well?',\n 'How can we extract terms from documents using LlamaIndex and insert them into the index?',\n 'What is the purpose of the HyDE query transform in the LlamaIndex?',\n 'What are the three primary sections within the layout of the ChatView component?',\n 'What is the default value for the child_branch_factor parameter when configuring the ComposableGraphQueryEngine?',\n 'What is the purpose of the `insert_terms` function in the LlamaIndex app?',\n 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?',\n 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?',\n 'What is the purpose of the `RouterQueryEngine` in LlamaIndex and how can it be used in the search pipeline?']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:59:10.020142Z",
     "start_time": "2024-02-27T12:59:09.941074Z"
    }
   },
   "id": "4fc82053453bec68",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate with the Dataset\n",
    "\n",
    "Now that we have our dataset, let's measure performance!\n",
    "<br>\n",
    "\n",
    "#### Evaluating Response for Hallucination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96f2a1c89074bc93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Response\n",
    "\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for response in responses:\n",
    "            eval_result = 1 if \"YES\" in evaluator.evaluate(response) else 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:11:07.599856Z",
     "start_time": "2024-02-27T13:11:07.598893Z"
    }
   },
   "id": "81a0fb175885df1",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "contexts and response must be provided",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# gpt-4 evaluator!\u001B[39;00m\n\u001B[1;32m      4\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m ResponseEvaluator(llm\u001B[38;5;241m=\u001B[39mllm)\n\u001B[0;32m----> 6\u001B[0m total_correct, all_results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_query_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_engine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHallucination? Scored \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_correct\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m out of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(question_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m questions correctly.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[20], line 26\u001B[0m, in \u001B[0;36mevaluate_query_engine\u001B[0;34m(evaluator, query_engine, questions)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfinished batch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(batch_size\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m5\u001B[39m)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m out of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(questions)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m5\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m responses:\n\u001B[0;32m---> 26\u001B[0m     eval_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYES\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     27\u001B[0m     total_correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m eval_result\n\u001B[1;32m     28\u001B[0m     all_results\u001B[38;5;241m.\u001B[39mappend(eval_result)\n",
      "File \u001B[0;32m~/Desktop/Development/PycharmProjects/LlamaIndex-experiments/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/base.py:62\u001B[0m, in \u001B[0;36mBaseEvaluator.evaluate\u001B[0;34m(self, query, response, contexts, **kwargs)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     51\u001B[0m     query: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m     55\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EvaluationResult:\n\u001B[1;32m     56\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run evaluation with query string, retrieved contexts,\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    and generated response string.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m    Subclasses can override this method to provide custom evaluation logic and\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;03m    take in additional arguments.\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcontexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Development/PycharmProjects/LlamaIndex-experiments/.venv/lib/python3.10/site-packages/nest_asyncio.py:30\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     28\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[0;32m~/Desktop/Development/PycharmProjects/LlamaIndex-experiments/.venv/lib/python3.10/site-packages/nest_asyncio.py:98\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     97\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent loop stopped before Future completed.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py:201\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__log_traceback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 201\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception_tb)\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:232\u001B[0m, in \u001B[0;36mTask.__step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    230\u001B[0m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[1;32m    231\u001B[0m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[0;32m--> 232\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mcoro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    234\u001B[0m         result \u001B[38;5;241m=\u001B[39m coro\u001B[38;5;241m.\u001B[39mthrow(exc)\n",
      "File \u001B[0;32m~/Desktop/Development/PycharmProjects/LlamaIndex-experiments/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/faithfulness.py:134\u001B[0m, in \u001B[0;36mFaithfulnessEvaluator.aevaluate\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39msleep(sleep_time_in_seconds)\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m contexts \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontexts and response must be provided\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    136\u001B[0m docs \u001B[38;5;241m=\u001B[39m [Document(text\u001B[38;5;241m=\u001B[39mcontext) \u001B[38;5;28;01mfor\u001B[39;00m context \u001B[38;5;129;01min\u001B[39;00m contexts]\n\u001B[1;32m    137\u001B[0m index \u001B[38;5;241m=\u001B[39m SummaryIndex\u001B[38;5;241m.\u001B[39mfrom_documents(docs)\n",
      "\u001B[0;31mValueError\u001B[0m: contexts and response must be provided"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import ResponseEvaluator\n",
    "\n",
    "# gpt-4 evaluator!\n",
    "evaluator = ResponseEvaluator(llm=llm)\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:44:03.927941Z",
     "start_time": "2024-02-27T13:43:55.472441Z"
    }
   },
   "id": "4bf09b48c35825b0",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Investigate Hallucinations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "203ad73ce2cced80"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m hallucinated_questions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(question_dataset)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mall_results\u001B[49m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(hallucinated_questions)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'all_results' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(all_results)\n",
    "print(hallucinated_questions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:45:02.119910Z",
     "start_time": "2024-02-27T13:45:01.979671Z"
    }
   },
   "id": "a6457b361c7c6951",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = query_engine.query('How well does Llama-index work with Django?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:53:33.239597Z",
     "start_time": "2024-02-27T13:53:25.274434Z"
    }
   },
   "id": "2c041eeb6b990a56",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-index works efficiently with Django by allowing for the integration of data loaders like GoogleDocsReader. Additionally, Llama-index can be configured to work efficiently with Django by turning existing data loaders into tools that agents can use, enabling the loading, indexing, and querying of data within a single tool call. This approach abstracts away complexities and provides an ad-hoc index to overcome prompt window limitations for API calls, making Llama-index a suitable choice for working with Django.\n",
      "-------\n",
      "> Source (Doc id: 24f74b93-c572-42a0-9db7-a304afaf617b): Sub question: Can Llama-index be integrated with Django?\n",
      "Response: Yes, Llama-index can be integrated with Django.\n",
      "\n",
      "> Source (Doc id: 96ae4336-45e4-49e7-82a1-9c2acac8c3c5): Sub question: What are the data loaders available for Llama-index when using Django?\n",
      "Response: Llama-index provides data loaders for GoogleDocsReader when using Django.\n",
      "\n",
      "> Source (Doc id: d8ec1daa-4619-45b6-ab0c-001058eba292): Sub question: How can Llama-index be configured to work efficiently with Django?\n",
      "Response: Llama-index can be configured to work efficiently with Django by turning any existing LlamaIndex data loader into a tool that an agent can use. This tool allows f...\n",
      "\n",
      "> Source (Doc id: fb078d51-9154-4318-860d-7366bbbd5c68): toolkit = LlamaToolkit(\n",
      "    index_configs=index_configs,\n",
      ")\n",
      "\n",
      "> Source (Doc id: 1e8cf17b-456d-4d6c-a6db-8ad67c091927): from llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n",
      "\n",
      "tool_config = IndexToolConfig(\n",
      "    query_engine=query_engine, \n",
      "    name=f\"Vector Index\",\n",
      "    description=f\"useful for when you want to answer queries about X\",\n",
      "    tool_kw...\n",
      "\n",
      "> Source (Doc id: 6bdf7ebe-a631-4953-a653-61d1a6502b72): from llama_index import download_loader\n",
      "\n",
      "GoogleDocsReader = download_loader('GoogleDocsReader')\n",
      "loader = GoogleDocsReader()\n",
      "documents = loader.load_data(document_ids=[...])\n",
      "\n",
      "> Source (Doc id: 4bbcee6c-ccac-473f-9a9e-613724101ecc): LlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n",
      "\n",
      "Under the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n",
      "\n",
      "- **Document stores**: where ingested document...\n",
      "\n",
      "> Source (Doc id: 82c95b09-82fe-4bb5-b1e8-3c52061bdf38): This tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. D...\n",
      "\n",
      "> Source (Doc id: 746262c1-b2af-4249-a6b2-80ddd6032fb1): Oftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using). \n",
      "\n",
      "To tackle this, we’ve provided...\n"
     ]
    }
   ],
   "source": [
    "print(str(response))\n",
    "print(\"-------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:53:34.589373Z",
     "start_time": "2024-02-27T13:53:34.578240Z"
    }
   },
   "id": "d71e71167211a34c",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9fdb8e67fd131fa3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
